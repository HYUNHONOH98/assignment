{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import re, os, string, random\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/datasets/shivamkushwaha/bbc-full-text-document-classification\n",
    "의 bbc text data 를 훈련 데이터 셋으로 사용\n",
    "\n",
    "전처리 작업을 통해 train data를 list 의 형태로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12893\n"
     ]
    }
   ],
   "source": [
    "devided_text =[]\n",
    "\n",
    "bbc_dataset = os.listdir('/home/hyunhonoh/project/ringle_assignment/bbc')\n",
    "\n",
    "for i in range(len(bbc_dataset)):\n",
    "    bbc_text = os.listdir('/home/hyunhonoh/project/ringle_assignment/bbc/' + bbc_dataset[i])\n",
    "    for j in range(len(bbc_text)):\n",
    "        f = open('/home/hyunhonoh/project/ringle_assignment/bbc/' + bbc_dataset[i] + '/'+ bbc_text[j], encoding='latin1')\n",
    "        F = f.readlines()\n",
    "        devided_text = devided_text + F\n",
    "\n",
    "while '\\n' in devided_text:\n",
    "    devided_text.remove('\\n')\n",
    "\n",
    "print(len(devided_text))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = devided_text[:3000]\n",
    "test_data = devided_text[3000:3500]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전처리 과정: \n",
    " 모두 소문자 / (숫자 제거, 불용어 제거 취소) / 공백 제거 / Treebank tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "General Motors (GM) saw its net profits fall 37% in the last quarter of 2004, as it continued to be hit by losses at its European operations.\n",
      "general motors) saw its net profits fall the last quarter 2004 continued hit losses its european operations.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "train_data_sent = []\n",
    "shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
    "\n",
    "\n",
    "# 여기까지 해서 모든 데이터를 한 문장씩 string 의 형태로 list 에 저장함\n",
    "for i in range(len(train_data)):\n",
    "    train_data_sent += sent_tokenize(train_data[i])\n",
    "print(type(train_data_sent[1]))\n",
    "\n",
    "print(train_data_sent[1])\n",
    "\n",
    "\n",
    "for i in range(len(train_data_sent)):\n",
    "    train_data_sent[i] = shortword.sub('', train_data_sent[i])\n",
    "    train_data_sent[i] = train_data_sent[i].lower()\n",
    "\n",
    "\n",
    "print(train_data_sent[1])\n",
    "# print(len(train_data_sent))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Compound sentence refers to a sentence made up of two independent clauses connected to one another with a coordinating conjunction, such as FANBOYS (for , and, nor, but, or, yet, so). \n",
    "-> COMPOUND 는 두 독립적인 문장이 합쳐져서 만들어진 문장으로, 접속사로 fanboys를 사용한다.\n",
    "\n",
    "2. COMPLEX SENTENCE is made up of an independent clause and one or more dependent clauses connected to it. Dependent clauses begin with subordinating conjunctions, such as (after, although, as, because, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 nltk를 이용해서 문장 단위로 분리해둔 단위 문장들을 spacy 의 postagging 기능을 이용해서 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm, en_core_web_trf\n",
    "nlp = en_core_web_sm.load()\n",
    "tokenized_list=[]\n",
    "doc =[]\n",
    "\n",
    "for i in range(len(train_data_sent)):\n",
    "    doc.append(nlp(train_data_sent[i]))\n",
    "    tokenized = [(token.text, token.dep_) for token in doc[i]]\n",
    "    tokenized_list.append(tokenized)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS_list의 인덱스값 = 9537\n",
      "tokenized_list의 인덱스 값 = 9537\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "POS_list=[] #POS list는 각 문장의 POS 값들을 모아놓은 리스트\n",
    "\n",
    "\n",
    "for j in range(len(tokenized_list)):\n",
    "    POS=[]\n",
    "    for i in range(len(tokenized_list[j])):\n",
    "        POS.append(tokenized_list[j][i][1])\n",
    "    POS_list.append(POS)\n",
    "\n",
    "#실제 문장의 수와 POS 모음이 일치하는지 확인\n",
    "print('POS_list의 인덱스값 = {}'.format(len(POS_list)))\n",
    "print('tokenized_list의 인덱스 값 = {}'.format(len(tokenized_list)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "구글링을 통해 스스로 판단한 simple, complex, compound의 정의를 생각해서 아래와 같이 스스로 데이터를 분류해봄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of simple_sent = 305\n",
      "number of compound_sent = 165\n",
      "number of complex_sent = 3470\n",
      "number of compl_compo_sent = 5597\n",
      "the trade deficit large part the latter.\n",
      "that pushed the month deficit $500.5bn.\n",
      "low-cost airlines hit eurotunnel\n",
      " shareholder revolt threw out the old board 2004.\n",
      "their economic targets became known the lisbon agenda.\n"
     ]
    }
   ],
   "source": [
    "simple_sent=[]\n",
    "complex_sent=[]\n",
    "compound_sent=[]\n",
    "compl_compo_sent=[]\n",
    "for i in range(len(POS_list)):\n",
    "    if  POS_list[i].count('aux') + POS_list[i].count('ROOT') ==1 \\\n",
    "        and POS_list[i].count('cc') ==0 \\\n",
    "        and POS_list[i].count('prep') == 0 \\\n",
    "        and POS_list[i].count('advmod') ==0 \\\n",
    "        and POS_list[i].count('mark') == 0 \\\n",
    "        and POS_list[i].count('punct') == 1 \\\n",
    "        and POS_list[i].count('acl')==0\\\n",
    "        and POS_list[i].count('advcl')==0:\n",
    "        simple_sent.append(train_data_sent[i])\n",
    "    elif POS_list[i].count('cc') > 0 \\\n",
    "        and POS_list[i].count('aux') ==2 \\\n",
    "        and POS_list[i].count('nsubj')==2:\n",
    "        compound_sent.append(train_data_sent[i])\n",
    "    elif (POS_list[i].count('mark') == 1 \\\n",
    "        or POS_list[i].count('advmod') ==1 \\\n",
    "        or POS_list[i].count('prep') ==1)\\\n",
    "        and POS_list[i].count('punct') < 3:\n",
    "        complex_sent.append(train_data_sent[i])\n",
    "    else :\n",
    "        compl_compo_sent.append(train_data_sent[i])\n",
    "\n",
    "\n",
    "\n",
    "print('number of simple_sent = {}'.format(len(simple_sent)))\n",
    "print('number of compound_sent = {}'.format(len(compound_sent)))\n",
    "print('number of complex_sent = {}'.format(len(complex_sent)))\n",
    "print('number of compl_compo_sent = {}'.format(len(compl_compo_sent)))\n",
    "\n",
    "for i in range(5):\n",
    "    print(simple_sent[i])\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train data set 자체의 수가 매우 부족한 상황이지만, 기준을 낮게 잡아서 데이터의 수를 무작정 늘리는 것보다는 train data set 이 깔끔한게 더 좋을 것으로 판단하였다.\n",
    "동일한 학습 환경을 조성하는 것이 옳다고 생각해서 우선 모든 train data set을 160개로 고정하도록 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_trainset = []\n",
    "compound_trainset = []\n",
    "complex_trainset = []\n",
    "compl_compo_trainset = []\n",
    "\n",
    "for i in range(160):\n",
    "    simple_trainset.append(simple_sent[i])\n",
    "    compound_trainset.append(compound_sent[i])\n",
    "    complex_trainset.append(complex_sent[i])\n",
    "    compl_compo_trainset.append(compl_compo_sent[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "미처 안한 불용어 제거와 punctuation 제거, 숫자 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prepro_simple_trainset =[]\n",
    "prepro_complex_trainset =[]\n",
    "prepro_compound_trainset =[]\n",
    "prepro_compl_compo_trainset =[]\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "for sentence in simple_trainset:\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)    \n",
    "    sentence = sentence.translate(translator)\n",
    "    token = tokenizer.tokenize(sentence)\n",
    "    result=[]\n",
    "    for word in token:\n",
    "        if word not in stop_words:\n",
    "            if len(word):\n",
    "                result.append(word)\n",
    "    prepro_simple_trainset.append(result)\n",
    "for sentence in complex_trainset:\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)    \n",
    "    sentence = sentence.translate(translator)\n",
    "    token = tokenizer.tokenize(sentence)\n",
    "    result=[]\n",
    "    for word in token:\n",
    "        if word not in stop_words:\n",
    "            if len(word):\n",
    "                result.append(word)\n",
    "    prepro_complex_trainset.append(result)\n",
    "for sentence in compound_trainset:\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "    sentence = sentence.translate(translator)\n",
    "    token = tokenizer.tokenize(sentence)\n",
    "    result=[]\n",
    "    for word in token:\n",
    "        if word not in stop_words:\n",
    "            if len(word):\n",
    "                result.append(word)\n",
    "    prepro_compound_trainset.append(result)\n",
    "for sentence in compl_compo_trainset:\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "    sentence = sentence.translate(translator)\n",
    "    token = tokenizer.tokenize(sentence)\n",
    "    result=[]\n",
    "    for word in token:\n",
    "        if word not in stop_words:\n",
    "            if len(word):\n",
    "                result.append(word)\n",
    "    prepro_compl_compo_trainset.append(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nltk 의 tokenizer를 이용해서 fit_on_text를 하여 각 문장에 전처리 후에 남은 분석에 사용하기가 용이안 단어들이 각각 얼마나 남았는지 판단\n",
    "texts_to_sequence를 통해 정수 인코딩 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_encode = Tokenizer()\n",
    "tokenizer_encode.fit_on_texts(prepro_simple_trainset + prepro_complex_trainset \\\n",
    "    + prepro_compound_trainset + prepro_compl_compo_trainset)\n",
    "\n",
    "encoded_simple_trainset = tokenizer_encode.texts_to_sequences(prepro_simple_trainset)\n",
    "encoded_compound_trainset = tokenizer_encode.texts_to_sequences(prepro_compound_trainset)\n",
    "encoded_complex_trainset = tokenizer_encode.texts_to_sequences(prepro_complex_trainset)\n",
    "encoded_compl_compo_trainset = tokenizer_encode.texts_to_sequences(prepro_compl_compo_trainset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMDB movie review dataset 과 유사하게 만들어주기 위해서 ndarray 형태로 만들어주고 이를 셔플해서 섞어줌."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2596, 2597, 2598, 2599, 2600, 1040, 622, 131, 1044]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot_trainset=[encoded_simple_trainset, encoded_compound_trainset, \\\n",
    "    encoded_complex_trainset, encoded_compl_compo_trainset]\n",
    "index=0\n",
    "data=[]\n",
    "label=[]\n",
    "for trainset in tot_trainset:\n",
    "    data.append(np.array([trainset[i] for i in range(160)], dtype=object))\n",
    "    label.append(np.full(160, index, dtype=np.int32))\n",
    "    index +=1\n",
    "\n",
    "total_data = np.concatenate(data, 0)\n",
    "total_label = np.concatenate(label, 0)\n",
    "\n",
    "# shuffle data\n",
    "\n",
    "shuffle_case=np.arange(total_data.shape[0])\n",
    "np.random.shuffle(shuffle_case)\n",
    "\n",
    "shuffle_data = total_data[shuffle_case]\n",
    "shuffle_label = total_label[shuffle_case]\n",
    "shuffle_data[3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문장의 최대 길이를 확인하여, 이에 맞게 padding을 씌워주도록 하여 데이터셋의 크기를 일정하게 만들어준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장의 최대 길이 : 26\n",
      "문장의 평균 길이 : 10.140625\n"
     ]
    }
   ],
   "source": [
    "data_length = [len(data) for data in shuffle_data]\n",
    "\n",
    "print('문장의 최대 길이 : {}'.format(np.max(data_length)))\n",
    "print('문장의 평균 길이 : {}'.format(np.mean(data_length)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델은 1D convolution 모델을 사용하였다.\n",
    "1. 모델의 전반적인 구조는 '딥러닝을 이용한 자연어처리 입문'을 참고하였다.\n",
    "\n",
    "2. 임베딩 레이어에서 정수 인코딩 + 패딩이 씌워진 데이터들을 임베딩 벡터의 형태로 만들어주는데, 이때 벡터의 차원 수는 256 으로 하였다.\n",
    "\n",
    "3. convolution layer는 학습의 정확도가 크게 높아지지 않아서 parameter의 수를 늘리기 위해 기존에 하나 있던 것을 두개로 늘렸는데, 큰 소용이 없었다.\n",
    "\n",
    "4. convolution layer 의 뉴런의 수는 256개로 설정하였고, 마지막의 denselayer 를 거친 후의 차원 수는 validation 에 용이하게 만들기 위해서 낮추었는데, \n",
    "한번에 많이 낮추게 되면 왠지 정보 손실이 일어날 것 같아서 두번 거쳤다.\n",
    "\n",
    "5. drop out layer를 중간에 넣어서 과도하게 튀는 값들을 잡고자 하였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, Dropout, GlobalMaxPooling1D, Dense\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 26)\n"
     ]
    }
   ],
   "source": [
    "max_len=26\n",
    "vocab_size = 3000\n",
    "embedding_dim = 256\n",
    "padded = pad_sequences(shuffle_data, maxlen=max_len)\n",
    "X_train_set =padded[:540]\n",
    "X_test_set = padded[540:]\n",
    "Y_train_set =shuffle_label[:540]\n",
    "Y_test_set =shuffle_label[540:]\n",
    "\n",
    "print(X_test_set.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델의 정확도는 크게 좋지 않았는데, 몇가지 이유에 대해서 생각해보자면,\n",
    "\n",
    "1. 너무 과도한 기준의 설정으로 인해 데이터셋의 절대적인 양이 줄어버렸다. - 데이터셋의 크기와 질은 딥러닝에서 가장 중요하다고 생각하므로, 아마 가장 큰 요인이 아닐까 싶다.\n",
    "\n",
    "2. 목적에 맞지 않는 전처리 - nlp 에서 sentiment analysis 등의 분야에서 흔하게 사용되는 전처리 과정을 모두 사용해보고자 하였는데, 문장 구조에 따라서 simple 인지 complex인지 compound 인지 구분하는 task 였기 때문에\n",
    "    필요하지 않은 전처리 과정을 거치면서 문장에 중요한 정보들이 많이 사라졌을 수 있겠다고 생각하였다.\n",
    "\n",
    "3. 모델의 부적합 - image data 에 대한 classifying을 더 많이 해보아서 더 익숙한 convolution model로 과제를 진행하였는데, 텍스트 데이터에 한해서는 RNN 기반 모델이 더 좋은 성능을 낼 수 있다는 생각을 하였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "16/17 [===========================>..] - ETA: 0s - loss: -2.7773 - acc: 0.1953\n",
      "Epoch 1: val_acc improved from -inf to 0.27000, saving model to best_model2.h5\n",
      "17/17 [==============================] - 1s 33ms/step - loss: -2.8310 - acc: 0.2000 - val_loss: -6.8617 - val_acc: 0.2700\n",
      "Epoch 2/20\n",
      "15/17 [=========================>....] - ETA: 0s - loss: -7.5245 - acc: 0.2729\n",
      "Epoch 2: val_acc did not improve from 0.27000\n",
      "17/17 [==============================] - 0s 22ms/step - loss: -7.7615 - acc: 0.2648 - val_loss: -6.8622 - val_acc: 0.2400\n",
      "Epoch 3/20\n",
      "16/17 [===========================>..] - ETA: 0s - loss: -7.7735 - acc: 0.2266\n",
      "Epoch 3: val_acc did not improve from 0.27000\n",
      "17/17 [==============================] - 0s 22ms/step - loss: -7.7658 - acc: 0.2296 - val_loss: -6.8622 - val_acc: 0.2400\n",
      "Epoch 4/20\n",
      "16/17 [===========================>..] - ETA: 0s - loss: -7.9224 - acc: 0.2734\n",
      "Epoch 4: val_acc did not improve from 0.27000\n",
      "17/17 [==============================] - 0s 22ms/step - loss: -7.7658 - acc: 0.2722 - val_loss: -6.8622 - val_acc: 0.2400\n",
      "Epoch 5/20\n",
      "16/17 [===========================>..] - ETA: 0s - loss: -7.7735 - acc: 0.2324\n",
      "Epoch 5: val_acc did not improve from 0.27000\n",
      "17/17 [==============================] - 0s 22ms/step - loss: -7.7658 - acc: 0.2370 - val_loss: -6.8622 - val_acc: 0.2400\n",
      "Epoch 5: early stopping\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Conv1D(filters=256, kernel_size=3, padding='valid', activation='relu'))\n",
    "model.add(Conv1D(filters=256, kernel_size=3, padding='valid', activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(5))\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n",
    "mc = ModelCheckpoint('best_model2.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(X_train_set, Y_train_set, epochs=20, validation_data=(X_test_set, Y_test_set), callbacks=[es, mc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "급하게 진행하느라 더 깊이 공부하고 과제를 진행하지 못한 점 죄송합니다. \n",
    "\n",
    "감사합니다! - 노현호 올림."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2bbe33b9bc5926acfec85e87a9be2d267a9712b2a20fb470a422a4d3aca5e126"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
